
Preprocessing data:
  Normalize: transforms data to between 0 and 1 (divides vector by length)
  Standardize: transforms data to mean of 0 and SD of 1 (subtract mean than divide by its SD)
  Center: removes some confounding?

Shuffling data: generally a good idea with independent samples
  Timeseries: generally don't shuffle, but can use out of sample for test OR multi train/test splits
    - https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/


Models
SVM parameters:
  cost (C): low is less penality of error term (generalizes better), higher C will overfit/stricter - uses more hyperplanes
    - C is 1 by default, consider decreasing it if there's many noisy observations
  gamma: degree of influence of single training example, low is neighborhood, high is far




Evaluation:
Bias: error due to erroneous or overly simplistic assumptions in the learning algorithim (underfitting)
Variance: error due to too much complexity in the learning algorithm (overfitting)
Recall: true positive rate- amout of positives model claims compared to actual number of positives
Precision: positive predictive value -amout of accurate positives model claims compared to number positives model actually claims
ROC curve: graph representation of contrast between true positive rate and false positive rate at various thresholds
L1 regularization: Laplacean prior, more binary/sparse with many variables being assigned 0 or 1
L2 regularization: gaussian prior, tends to spread error among all terms (more typical)
Type I error: false positive - claiming something happened when it didn't
Type II error: false negative -claiming nothing happened when it did
Fourier transform: generic method to decompose generic functions into a superposition of symmetric functions
  - finds the set of cycle speeds, amplitudes and phases to match any time series
  
  
  tips+tricks:
  https://karpathy.github.io/2019/04/25/recipe/
